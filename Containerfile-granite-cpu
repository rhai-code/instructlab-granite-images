FROM registry.access.redhat.com/ubi9:9.5-1734081738 AS env-build

WORKDIR /srv

# ARG MODEL_DOWNLOAD_URL

ARG MODEL_DOWNLOAD_URL
ARG MODEL_FILENAME

# Set LLAMA.CPP SHA for git pull
ARG LLAMA_CPP_SHA=1329c0a75e6a7defc5c380eaf80d8e0f66d7da78
# vulkan-headers vulkan-loader-devel vulkan-tools glslc glslang python3-pip mesa-libOpenCL-$MESA_VER.aarch64
RUN dnf install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-9.noarch.rpm && \
    crb enable && \
    dnf install -y epel-release && \
    dnf --enablerepo=ubi-9-appstream-rpms install -y git procps-ng vim \
      dnf-plugins-core python3-dnf-plugin-versionlock cmake gcc-c++ \
      python3-pip python3-argcomplete && \
    dnf copr enable -y slp/mesa-krunkit epel-9-$(uname -m) && \
    dnf install -y mesa-vulkan-drivers-23.3.3-102.el9 \
      vulkan-headers vulkan-loader-devel vulkan-tools spirv-tools glslc && \
    dnf clean all && \
    rm -rf /var/cache/*dnf*

# install build tools and clone and compile llama.cpp
RUN dnf -y update && dnf install -y git make automake gcc gcc-c++ llvm-toolset wget

RUN git clone --recursive https://github.com/ggerganov/llama.cpp && \
    cd llama.cpp && \
    git reset --hard ${LLAMA_CPP_SHA} && \
    cmake -B build -DCMAKE_INSTALL_PREFIX:PATH=/usr -DGGML_KOMPUTE=1 \
      -DGGML_CCACHE=0 && \
    cmake --build build --config Release -j $(nproc) && \
    cmake --install build && \
    cd / && \
    rm -rf llama.cpp


RUN --mount=type=secret,id=hf-token/token wget --header="Authorization: Bearer $(cat /run/secrets/hf-token/token)" ${MODEL_DOWNLOAD_URL} -P /models

ENV LD_LIBRARY_PATH=/usr/local/lib

ENV MODEL_PATH=/models/${MODEL_FILENAME}
EXPOSE 8080

# copy and set entrypoint script
COPY entrypoint.sh /home/llama/entrypoint.sh

COPY granite-license /licenses/granite-license
COPY llama-cpp-license /licenses/llama-cpp-license

RUN ["chmod", "+x", "/home/llama/entrypoint.sh"]

RUN useradd --system --create-home llama

USER llama

WORKDIR /home/llama

ENTRYPOINT [ "/home/llama/entrypoint.sh" ]